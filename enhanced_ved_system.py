#!/usr/bin/env python3
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –í–≠–î –≠–∫—Å–ø–µ—Ä—Ç —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
"""

import json
import re
import hashlib
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ved_expert.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('VEDExpert')

@dataclass
class SearchResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ–∏—Å–∫–∞ —Ç–æ–≤–∞—Ä–∞"""
    product: Optional[Dict]
    confidence: float
    search_type: str  # 'code', 'name', 'fuzzy'
    
@dataclass 
class VEDAnalysis:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –í–≠–î —Å–∏—Å—Ç–µ–º—É"""
    official_data: Dict
    genspark_analysis: str
    confidence: float
    sources: List[str]

class EnhancedCache:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    
    def __init__(self, ttl_minutes=60):
        self.cache = {}
        self.ttl = timedelta(minutes=ttl_minutes)
        self.stats = {"hits": 0, "misses": 0}
        logger.info(f"Cache initialized with TTL: {ttl_minutes} minutes")
    
    def _generate_key(self, query: str) -> str:
        return hashlib.md5(query.lower().encode()).hexdigest()
    
    def get(self, query: str) -> Optional[Dict]:
        key = self._generate_key(query)
        if key in self.cache:
            timestamp, value = self.cache[key]
            if datetime.now() - timestamp < self.ttl:
                self.stats["hits"] += 1
                logger.debug(f"Cache HIT for query: {query[:20]}...")
                return value
            else:
                del self.cache[key]
        
        self.stats["misses"] += 1
        logger.debug(f"Cache MISS for query: {query[:20]}...")
        return None
    
    def set(self, query: str, value: Dict):
        key = self._generate_key(query)
        self.cache[key] = (datetime.now(), value)
        logger.debug(f"Cached result for query: {query[:20]}...")

class SmartQueryParser:
    """–£–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self):
        self.tnved_pattern = re.compile(r'\b\d{10}\b')
        self.hs_pattern = re.compile(r'\b\d{4,6}\b')
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è
        self.synonyms = {
            '–Ω–æ—É—Ç–±—É–∫': ['–Ω–æ—É—Ç–±—É–∫', '–ª—ç–ø—Ç–æ–ø', 'laptop', '–ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä'],
            '—Ç–µ–ª–µ—Ñ–æ–Ω': ['—Ç–µ–ª–µ—Ñ–æ–Ω', '—Å–º–∞—Ä—Ç—Ñ–æ–Ω', '–º–æ–±–∏–ª—å–Ω—ã–π', 'phone', 'smartphone'],
            '–∞–≤—Ç–æ–º–æ–±–∏–ª—å': ['–∞–≤—Ç–æ–º–æ–±–∏–ª—å', '–º–∞—à–∏–Ω–∞', '–∞–≤—Ç–æ', 'car', '–ª–µ–≥–∫–æ–≤–æ–π –∞–≤—Ç–æ–º–æ–±–∏–ª—å'],
            '—Å–≤–∏–Ω–∏–Ω–∞': ['—Å–≤–∏–Ω–∏–Ω–∞', '–º—è—Å–æ —Å–≤–∏–Ω—å–∏', '—Å–≤–∏–Ω–æ–µ –º—è—Å–æ', 'pork'],
            '–≥–æ–≤—è–¥–∏–Ω–∞': ['–≥–æ–≤—è–¥–∏–Ω–∞', '–º—è—Å–æ –≥–æ–≤—è–¥–∏–Ω—ã', 'beef', '—Ç–µ–ª—è—Ç–∏–Ω–∞'],
            '–º–æ–ª–æ–∫–æ': ['–º–æ–ª–æ–∫–æ', 'milk', '–º–æ–ª–æ—á–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã'],
            '—Ä—ã–±–∞': ['—Ä—ã–±–∞', 'fish', '—Ä—ã–±–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã'],
            '–∫–æ–º–ø—å—é—Ç–µ—Ä': ['–∫–æ–º–ø—å—é—Ç–µ—Ä', '–ø–∫', 'pc', 'computer']
        }
        
        logger.info("SmartQueryParser initialized with extended synonyms")
    
    def parse_query(self, query: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –ø–æ–∏—Å–∫–∞"""
        query_lower = query.lower().strip()
        
        result = {
            'original_query': query,
            'normalized_query': query_lower,
            'search_type': 'unknown',
            'codes': [],
            'keywords': [],
            'confidence': 0.0
        }
        
        # –ü–æ–∏—Å–∫ –∫–æ–¥–æ–≤ –¢–ù –í–≠–î
        tnved_codes = self.tnved_pattern.findall(query)
        if tnved_codes:
            result['search_type'] = 'tnved_code'
            result['codes'] = tnved_codes
            result['confidence'] = 1.0
            logger.info(f"Detected TNVED codes: {tnved_codes}")
            return result
        
        # –ü–æ–∏—Å–∫ HS –∫–æ–¥–æ–≤
        hs_codes = self.hs_pattern.findall(query)
        if hs_codes:
            result['search_type'] = 'hs_code'
            result['codes'] = hs_codes
            result['confidence'] = 0.8
            logger.info(f"Detected HS codes: {hs_codes}")
            return result
        
        # –ü–æ–∏—Å–∫ –ø–æ —Å–∏–Ω–æ–Ω–∏–º–∞–º
        matched_categories = []
        for category, synonyms in self.synonyms.items():
            for synonym in synonyms:
                if synonym in query_lower:
                    matched_categories.append(category)
                    result['keywords'].append(synonym)
                    break
        
        if matched_categories:
            result['search_type'] = 'category_match'
            result['keywords'] = matched_categories
            result['confidence'] = 0.9
            logger.info(f"Matched categories: {matched_categories}")
            return result
        
        # –û–±—â–∏–π —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
        result['search_type'] = 'text_search'
        result['keywords'] = query_lower.split()
        result['confidence'] = 0.5
        logger.info(f"Text search with keywords: {result['keywords']}")
        
        return result

class EnhancedVEDDatabase:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –¢–ù –í–≠–î —Å —É–º–Ω—ã–º –ø–æ–∏—Å–∫–æ–º"""
    
    def __init__(self, data_path=None):
        if data_path is None:
            data_path = Path(__file__).parent
        else:
            data_path = Path(data_path)
        
        self.data_path = data_path
        self.database = {}
        self.cache = EnhancedCache()
        self.parser = SmartQueryParser()
        
        self._load_database()
        logger.info("EnhancedVEDDatabase initialized successfully")
    
    def _load_database(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        try:
            db_file = self.data_path / "tnved_database.json"
            with open(db_file, "r", encoding="utf-8") as f:
                self.database = json.load(f)
            
            codes_count = len(self.database.get("codes", []))
            logger.info(f"Database loaded: {codes_count} products")
            
        except FileNotFoundError as e:
            logger.error(f"Database file not found: {e}")
            self.database = {"codes": [], "groups": []}
        except Exception as e:
            logger.error(f"Error loading database: {e}")
            self.database = {"codes": [], "groups": []}
    
    def smart_search(self, query: str) -> SearchResult:
        """–£–º–Ω—ã–π –ø–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–æ–≤"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
        cached_result = self.cache.get(query)
        if cached_result:
            return SearchResult(**cached_result)
        
        # –ü–∞—Ä—Å–∏–º –∑–∞–ø—Ä–æ—Å
        parsed = self.parser.parse_query(query)
        result = None
        confidence = 0.0
        search_type = parsed['search_type']
        
        # –ü–æ–∏—Å–∫ –ø–æ —Ç–∏–ø—É –∑–∞–ø—Ä–æ—Å–∞
        if search_type == 'tnved_code':
            result = self._search_by_code(parsed['codes'][0])
            confidence = 1.0 if result else 0.0
            
        elif search_type == 'category_match':
            result = self._search_by_category(parsed['keywords'])
            confidence = 0.9 if result else 0.0
            
        elif search_type == 'text_search':
            result = self._search_by_text(parsed['keywords'])
            confidence = 0.7 if result else 0.0
        
        search_result = SearchResult(
            product=result,
            confidence=confidence,
            search_type=search_type
        )
        
        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache.set(query, {
            'product': result,
            'confidence': confidence,
            'search_type': search_type
        })
        
        logger.info(f"Search completed: {search_type}, confidence: {confidence}")
        return search_result
    
    def _search_by_code(self, code: str) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–æ–¥—É –¢–ù –í–≠–î"""
        for product in self.database.get("codes", []):
            if product.get("code") == code:
                return product
        return None
    
    def _search_by_category(self, keywords: List[str]) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–∞"""
        for product in self.database.get("codes", []):
            product_name = product.get("name", "").lower()
            for keyword in keywords:
                if keyword in product_name:
                    return product
        return None
    
    def _search_by_text(self, keywords: List[str]) -> Optional[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ —Ç–µ–∫—Å—Ç—É"""
        best_match = None
        best_score = 0
        
        for product in self.database.get("codes", []):
            product_text = f"{product.get('name', '')} {product.get('description', '')}".lower()
            
            score = 0
            for keyword in keywords:
                if keyword in product_text:
                    score += 1
            
            if score > best_score:
                best_score = score
                best_match = product
        
        return best_match if best_score > 0 else None

class GensparktVEDIntegration:
    """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Genspark –¥–ª—è –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"""
    
    def __init__(self, genspark_agent):
        self.genspark_agent = genspark_agent
        logger.info("GensparktVEDIntegration initialized")
    
    def analyze_with_context(self, official_data: Dict, user_query: str) -> VEDAnalysis:
        """–ê–Ω–∞–ª–∏–∑ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        if not official_data:
            # –ï—Å–ª–∏ –Ω–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Genspark
            logger.warning("No official data found, using Genspark only")
            genspark_result = self._get_genspark_analysis(user_query)
            
            return VEDAnalysis(
                official_data={},
                genspark_analysis=genspark_result,
                confidence=0.3,
                sources=["Genspark AI"]
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Genspark
        context = self._format_official_context(official_data)
        enhanced_query = f"""
–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¢–ù –í–≠–î:
{context}

–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: {user_query}

–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏ –¥–æ–ø–æ–ª–Ω–∏ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é:
1. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
2. –î–æ–±–∞–≤—å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Å–æ–≤–µ—Ç—ã –ø–æ –∏–º–ø–æ—Ä—Ç—É/—ç–∫—Å–ø–æ—Ä—Ç—É
3. –£–∫–∞–∂–∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã –∏–ª–∏ –ø–æ—Ö–æ–∂–∏–µ —Ç–æ–≤–∞—Ä—ã
4. –ü—Ä–µ–¥—É–ø—Ä–µ–¥–∏ –æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç—è—Ö
"""
        
        genspark_analysis = self._get_genspark_analysis(enhanced_query)
        
        return VEDAnalysis(
            official_data=official_data,
            genspark_analysis=genspark_analysis,
            confidence=0.95,
            sources=["–û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è –±–∞–∑–∞ –¢–ù –í–≠–î", "Genspark AI"]
        )
    
    def _format_official_context(self, data: Dict) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        context_parts = [
            f"–ö–æ–¥ –¢–ù –í–≠–î: {data.get('code', '–ù/–î')}",
            f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ: {data.get('name', '–ù/–î')}",
            f"–û–ø–∏—Å–∞–Ω–∏–µ: {data.get('description', '–ù/–î')}",
            f"–ì—Ä—É–ø–ø–∞: {data.get('group', '–ù/–î')}"
        ]
        
        duties = data.get('duties', {})
        if duties:
            duties_text = "–ü–æ—à–ª–∏–Ω—ã: " + ", ".join([f"{k}: {v}%" for k, v in duties.items()])
            context_parts.append(duties_text)
        
        cert = data.get('certification', [])
        if cert:
            context_parts.append(f"–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è: {', '.join(cert)}")
        
        restrictions = data.get('restrictions', [])
        if restrictions:
            context_parts.append(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: {', '.join(restrictions)}")
        
        return "\n".join(context_parts)
    
    def _get_genspark_analysis(self, query: str) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –æ—Ç Genspark"""
        try:
            # –ó–¥–µ—Å—å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≤—ã–∑–æ–≤ –∫ –≤–∞—à–µ–º—É Genspark –∞–≥–µ–Ω—Ç—É
            # result = self.genspark_agent.analyze(query)
            
            # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            result = f"–ê–Ω–∞–ª–∏–∑ Genspark AI –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞: {query[:50]}... (–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º –∞–≥–µ–Ω—Ç–æ–º)"
            logger.info("Genspark analysis completed")
            return result
            
        except Exception as e:
            logger.error(f"Genspark analysis failed: {e}")
            return "–ê–Ω–∞–ª–∏–∑ Genspark AI –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"

def format_enhanced_response(analysis: VEDAnalysis) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é"""
    
    blocks = []
    
    # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    if analysis.official_data:
        official_block = format_official_data(analysis.official_data)
        blocks.append(f"üìä *–û–§–ò–¶–ò–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï –¢–ù –í–≠–î:*\n{official_block}")
    
    # –ê–Ω–∞–ª–∏–∑ Genspark
    if analysis.genspark_analysis:
        blocks.append(f"üß† *–≠–ö–°–ü–ï–†–¢–ù–´–ô –ê–ù–ê–õ–ò–ó:*\n{analysis.genspark_analysis}")
    
    # –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å
    confidence_text = f"üéØ *–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å:* {analysis.confidence*100:.0f}%"
    sources_text = f"üìö *–ò—Å—Ç–æ—á–Ω–∏–∫–∏:* {', '.join(analysis.sources)}"
    blocks.extend([confidence_text, sources_text])
    
    return "\n\n".join(blocks)

def format_official_data(data: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    lines = [
        f"üìå **{data.get('name', '–¢–æ–≤–∞—Ä')}**",
        f"üî¢ –ö–æ–¥: `{data.get('code', '–ù/–î')}`",
        f"üìù {data.get('description', '–û–ø–∏—Å–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç')}"
    ]
    
    duties = data.get('duties', {})
    if duties:
        duties_lines = ["üí∞ **–ü–æ—à–ª–∏–Ω—ã:**"]
        for country, rate in duties.items():
            country_name = {
                'base': '–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞',
                'china': '–ö–∏—Ç–∞–π',
                'eu': '–ï–°',
                'usa': '–°–®–ê'
            }.get(country, country.capitalize())
            duties_lines.append(f"  ‚Ä¢ {country_name}: {rate}%")
        lines.extend(duties_lines)
    
    cert = data.get('certification', [])
    if cert:
        lines.append(f"üìÑ **–°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è:** {', '.join(cert)}")
    
    restrictions = data.get('restrictions', [])
    if restrictions:
        lines.append(f"‚ö†Ô∏è **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** {', '.join(restrictions)}")
    else:
        lines.append("‚úÖ **–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:** –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
    
    return "\n".join(lines)

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å —Å–∏—Å—Ç–µ–º—ã
class EnhancedVEDExpertSystem:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –í–≠–î –≠–∫—Å–ø–µ—Ä—Ç"""
    
    def __init__(self, genspark_agent=None):
        self.database = EnhancedVEDDatabase()
        self.genspark_integration = GensparktVEDIntegration(genspark_agent) if genspark_agent else None
        logger.info("EnhancedVEDExpertSystem initialized")
    
    def process_query(self, query: str) -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        logger.info(f"Processing query: {query[:50]}...")
        
        # –ü–æ–∏—Å–∫ –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –±–∞–∑–µ
        search_result = self.database.smart_search(query)
        
        # –ê–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ Genspark (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.genspark_integration:
            analysis = self.genspark_integration.analyze_with_context(
                search_result.product, query
            )
            response = format_enhanced_response(analysis)
        else:
            # –¢–æ–ª—å–∫–æ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            if search_result.product:
                response = f"üìä *–ù–∞–π–¥–µ–Ω —Ç–æ–≤–∞—Ä:*\n{format_official_data(search_result.product)}"
            else:
                response = "‚ùå –¢–æ–≤–∞—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–π –±–∞–∑–µ –¢–ù –í–≠–î"
        
        logger.info(f"Query processed, confidence: {search_result.confidence}")
        return response

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã
    system = EnhancedVEDExpertSystem()
    
    test_queries = [
        "–Ω–æ—É—Ç–±—É–∫",
        "8471300000", 
        "—Å–≤–∏–Ω–∏–Ω–∞",
        "–∞–≤—Ç–æ–º–æ–±–∏–ª—å –ª–µ–≥–∫–æ–≤–æ–π"
    ]
    
    for query in test_queries:
        print(f"\n–ó–∞–ø—Ä–æ—Å: {query}")
        print("-" * 50)
        result = system.process_query(query)
        print(result)
        print("=" * 50)